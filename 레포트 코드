
#  패키지
import numpy as np
import pandas as pd
import scipy as sp
import scipy.stats as stats
import statsmodels.api as sm
from statsmodels.formula.api import ols
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 데이터
df=pd.read_excel('data.xlsx')
X = pd.DataFrame(df, columns=['Cel', 'Hem', 'Lig'])
y = pd.DataFrame(df, columns=['SMY'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2023)
normalizer=StandardScaler()
X_train=normalizer.fit_transform(X_train)
X_test=normalizer.transform(X_test)

# 모델 구축
X_train = sm.add_constant(X_train)
model = sm.OLS(y_train, X_train)
model_trained = model.fit()

[부록 코드 2] 다항회귀
# 패키지
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 데이터: 앞과 동일

# 모델 구축
degree=3
poly_model = PolynomialFeatures(degree=degree)
poly_X_train = poly_model.fit_transform(X_train)
regression_model = LinearRegression()
regression_model.fit(poly_X_train, y_train)

[부록 코드 3] Elastic Net
# 패키지
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import ElasticNet
from sklearn.linear_model import ElasticNetCV
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RepeatedKFold

# 데이터: 앞과 동일

# 모델 구축
enet = ElasticNet()
grid = dict()
grid['alpha'] = np.arange(0, 5, 0.1)
grid['l1_ratio'] = np.arange(0, 1, 0.1)
search = GridSearchCV(enet, grid, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)
results = search.fit(X_train, y_train)

[부록 코드 4] 인공 신경망
# 패키지
from sklearn.neural_network import MLPRegressor
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
import pandas as pd
from sklearn.preprocessing import StandardScaler

# 데이터: 앞과 동일

# 모델 구축
param = {
    'hidden_layer_sizes': [(100, 64, 64, 1)],
    'activation': ['relu'],
    'learning_rate': ['constant']
}
mlp_reg = MLPRegressor()
mlp_reg.set_params(param) 
mlp_reg.fit(X_train, y_train)

[부록 코드 5] SVR
# 패키지
import pandas as pd
import numpy as np
from sklearn.svm import SVR
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import scale, StandardScaler

# 데이터: 앞과 동일

# 모델 구축
svr = SVR(C=1000, kernel='rbf', gamma='auto')
svr.fit(X_train, y_train)

[부록 코드 6] 랜덤 포레스트
# 패키지
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

# 데이터: 앞과 동일

# 모델 구축
rfr = RandomForestRegressor(random_state=2023)
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [2,4],
    'criterion': ['mae']
}
CV_rfr = GridSearchCV(estimator=rfr, param_grid=param_grid, cv=5, verbose=1 , n_jobs=-1, scoring='neg_mean_squared_error')
CV_rfr.fit(X_train, y_train)
best_rfr = CV_rfr.best_estimator_
best_rfr.fit(X_train, y_train)

[부록 코드 7] 가우시안 프로세스
# 패키지
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF

# 데이터: 앞과 동일

# 모델 구축
kernel = 1*RBF(length_scale=1)
model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=0, alpha=1000)
model.fit(X_train, y_train)

[부록 코드 8] k-NN
# 패키지
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neighbors import NearestNeighbors

# 데이터: 앞과 동일

# 모델 구축
regressor_euc = KNeighborsRegressor(n_neighbors = 5, p=2, weights='distance')
regressor_euc.fit(X_train, y_train)

[부록 코드 9] 앙상블
# 패키지
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import RepeatedKFold
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import plot_confusion_matrix
from sklearn.model_selection import KFold
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neighbors import NearestNeighbors
from sklearn.ensemble import RandomForestClassifier, VotingRegressor
from sklearn.linear_model import LinearRegression

import scipy as sp
import scipy.stats as stats

import statsmodels.api as sm
from statsmodels.formula.api import ols

# 데이터: 앞과 동일

# 모델 구축
clf1 = RandomForestRegressor( n_estimators=150, random_state=2023, max_depth= 5, min_samples_split= 5, min_samples_leaf= 2, criterion= 'mae')
kernel = 1*RBF(length_scale=1)
clf2 = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=0, alpha=1000)
clf3 = KNeighborsRegressor(n_neighbors = 5, p=2, weights='distance')
clf4 = SVR(C=1000, kernel='rbf', gamma='auto')
clf5  = LinearRegression()
eclf1 = VotingRegressor(estimators=[
        ('knn', clf3), ('gpr', clf2), ('svr', clf4), ('random', clf1), ('linear', clf5)])
eclf1=eclf1.fit(X_train, y_train)
